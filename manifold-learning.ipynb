{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning Techinques for NLP Embeddings\n",
    "#### This notebook contains code to visulize BERT embeddings in 3D using manifold learning techniques which in contrast to linear projection of data, look at non-linear structure in embeddings.\n",
    "\n",
    "\n",
    "##### More on: [Scikit-learn Manifold Learning Techniques](https://scikit-learn.org/1.5/modules/manifold.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.manifold import (\n",
    "    Isomap,\n",
    "    LocallyLinearEmbedding,\n",
    "    MDS,\n",
    "    SpectralEmbedding,\n",
    "    TSNE,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset read and \n",
    "df = pd.read_csv(\"./ner_datasetreference.csv\", encoding= 'unicode_escape')\n",
    "df = df.iloc[:109894]\n",
    "unique_POS = pd.unique(df[['POS']].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentence #'].fillna(method='ffill', inplace=True)\n",
    "df['Complete Sentence'] = df.groupby('Sentence #')['Word'].transform(lambda x: ' '.join(x.dropna()))\n",
    "df = df[df['Word'].notna()]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_name = \"vblagoje/bert-english-uncased-finetuned-pos\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[\"Complete Sentence\"].unique()\n",
    "\n",
    "id2lbel = {\n",
    "    \"0\": \"ADJ\",\n",
    "    \"1\": \"ADP\",\n",
    "    \"2\": \"ADV\",\n",
    "    \"3\": \"AUX\",\n",
    "    \"4\": \"CCONJ\",\n",
    "    \"5\": \"DET\",\n",
    "    \"6\": \"INTJ\",\n",
    "    \"7\": \"NOUN\",\n",
    "    \"8\": \"NUM\",\n",
    "    \"9\": \"PART\",\n",
    "    \"10\": \"PRON\",\n",
    "    \"11\": \"PROPN\",\n",
    "    \"12\": \"PUNCT\",\n",
    "    \"13\": \"SCONJ\",\n",
    "    \"14\": \"SYM\",\n",
    "    \"15\": \"VERB\",\n",
    "    \"16\": \"X\"\n",
    "  }\n",
    "\n",
    "output_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tqdm(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokenized_text = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "    # Encode the tokens\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    # Pass the input through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "\n",
    "    # print(outputs.keys())\n",
    "\n",
    "    # Predicted POS tags\n",
    "    predicted_tags = torch.argmax(outputs.logits, dim=2).squeeze(0)\n",
    "\n",
    "    # Last layer hidden states\n",
    "    last_layer_hidden_states = outputs.hidden_states[-1][0]\n",
    "\n",
    "    # Remove details of extra special tokens\n",
    "    tokens = tokenized_text[1:-1]  # Remove [CLS] and [SEP]\n",
    "    predicted_tags = predicted_tags[1:-1]  # Remove tags for [CLS] and [SEP]\n",
    "    embeddings = last_layer_hidden_states[1:-1]  # Remove embeddings for [CLS] and [SEP]\n",
    "\n",
    "    for token, tag_idx, embedding in zip(tokens, predicted_tags, embeddings):\n",
    "        output_list.append([token, id2lbel[str(tag_idx.item())], embedding.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = pd.DataFrame(output_list, columns =['Token', 'POS', 'Embedding'])\n",
    "output_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropping data to reduce processing time\n",
    "# smaller_chunk_df = output_dataframe[:10000]\n",
    "\n",
    "# cropping based on tokens potentially prone to bias\n",
    "bias_related_tokens = [\n",
    "    # Race & Ethnicity\n",
    "    \"black\", \"white\", \"asian\", \"hispanic\", \"latino\", \"latina\", \"african\", \"european\", \"indian\", \"native\",\n",
    "    \"indigenous\", \"middle eastern\", \"arab\", \"jewish\", \"gypsy\", \"caucasian\",\n",
    "\n",
    "    # Religion\n",
    "    \"muslim\", \"christian\", \"jew\", \"hindu\", \"buddhist\", \"sikh\", \"atheist\", \"agnostic\", \"catholic\", \"protestant\",\n",
    "    \"orthodox\", \"islamic\", \"secular\", \"cleric\", \"rabbi\", \"imam\", \"monk\", \"priest\", \"pope\", \"missionary\",\n",
    "\n",
    "    # Crime & Violence\n",
    "    \"terrorism\", \"terrorist\", \"extremist\", \"radical\", \"bomb\", \"explosion\", \"attack\", \"war\", \"violence\",\n",
    "    \"crime\", \"criminal\", \"thief\", \"robbery\", \"murder\", \"homicide\", \"assault\", \"gang\", \"drug\", \"cartel\",\n",
    "    \"weapon\", \"gun\", \"knife\", \"arrest\", \"prison\", \"jail\", \"court\", \"lawyer\", \"judge\", \"felon\",\n",
    "\n",
    "    # Wealth & Social Class\n",
    "    \"rich\", \"poor\", \"wealthy\", \"middle class\", \"homeless\", \"privileged\", \"underprivileged\", \"elite\",\n",
    "    \"billionaire\", \"millionaire\", \"poverty\", \"welfare\", \"donor\", \"charity\",\n",
    "\n",
    "    # Gender & Sexuality\n",
    "    \"man\", \"woman\", \"boy\", \"girl\", \"male\", \"female\", \"nonbinary\", \"transgender\", \"gay\", \"lesbian\", \"bisexual\",\n",
    "    \"straight\", \"queer\", \"feminist\", \"misogyny\", \"patriarchy\", \"matriarchy\",\n",
    "\n",
    "    # Professions & Status\n",
    "    \"doctor\", \"nurse\", \"scientist\", \"engineer\", \"teacher\", \"professor\", \"researcher\", \"ceo\", \"manager\",\n",
    "    \"politician\", \"president\", \"prime minister\", \"soldier\", \"activist\", \"journalist\", \"lawyer\",\n",
    "\n",
    "    # Immigration & Nationality\n",
    "    \"refugee\", \"immigrant\", \"migrant\", \"illegal\", \"citizen\", \"foreigner\", \"passport\", \"asylum\", \"visa\",\n",
    "    \"border\", \"deportation\",\n",
    "\n",
    "    # Political Affiliation & Ideology\n",
    "    \"democrat\", \"republican\", \"liberal\", \"conservative\", \"socialist\", \"communist\", \"capitalist\", \"fascist\",\n",
    "    \"anarchist\", \"dictator\", \"monarchy\", \"feminism\", \"nationalist\", \"globalist\",\n",
    "\n",
    "    # Other Societal Factors\n",
    "    \"education\", \"intelligence\", \"genius\", \"dumb\", \"success\", \"failure\", \"addiction\", \"mental health\",\n",
    "    \"therapy\", \"disability\", \"autism\", \"depression\", \"anxiety\", \"bipolar\", \"schizophrenia\"\n",
    "]\n",
    "\n",
    "\n",
    "bias_related_tokens = set(map(str.lower, bias_related_tokens))\n",
    "smaller_chunk_df = output_dataframe[output_dataframe['Token'].str.lower().isin(bias_related_tokens)]\n",
    "print(smaller_chunk_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_chunk_df[\"Token\"].value_counts().reset_index()\n",
    "token, X, y = smaller_chunk_df[\"Token\"], smaller_chunk_df[\"Embedding\"].values.tolist(), smaller_chunk_df[\"POS\"].to_numpy()\n",
    "X = np.array(X)\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 60 # hyperparameter for manifold learning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "## Random projection embedding\n",
    "sp = SparseRandomProjection(\n",
    "        n_components=3, random_state=42\n",
    "    )\n",
    "X_sp = sp.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = {\n",
    "    \"First dimension\": X_sp[:, 0],\n",
    "    \"Second dimension\": X_sp[:, 1],\n",
    "    \"Third dimension\": X_sp[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_random, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=600, size_max=2, title=\"Random Projection\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isomap Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## Isomap embedding\n",
    "iso = Isomap(n_neighbors=n_neighbors, n_components=3)\n",
    "X_iso = iso.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = {\n",
    "    \"First dimension\": X_iso[:, 0],\n",
    "    \"Second dimension\": X_iso[:, 1],\n",
    "    \"Third dimension\": X_iso[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_1, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=800, size_max=2, title=\"ISOmap\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard LLE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## Standard LLE embedding\n",
    "lle_s = LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=3, method=\"standard\"\n",
    "    )\n",
    "X_lle_s = lle_s.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = {\n",
    "    \"First dimension\": X_lle_s[:, 0],\n",
    "    \"Second dimension\": X_lle_s[:, 1],\n",
    "    \"Third dimension\": X_lle_s[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_2, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=800, size_max=2, title=\"Standard LLE embedding\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified LLE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## Modified LLE embedding\n",
    "lle_m = LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=3, method=\"modified\"\n",
    "    )\n",
    "X_lle_m = lle_m.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = {\n",
    "    \"First dimension\": X_lle_m[:, 0],\n",
    "    \"Second dimension\": X_lle_m[:, 1],\n",
    "    \"Third dimension\": X_lle_m[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_3, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=800, size_max=2, title=\"Modified LLE embedding\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDS Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## MDS embedding\n",
    "mds = MDS(\n",
    "        n_components=3, normalized_stress=\"auto\"\n",
    "    )\n",
    "X_mds = mds.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = {\n",
    "    \"First dimension\": X_mds[:, 0],\n",
    "    \"Second dimension\": X_mds[:, 1],\n",
    "    \"Third dimension\": X_mds[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_4, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=800, size_max=2, title=\"MDS embedding\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Trees embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## Random Trees embedding\n",
    "rte = make_pipeline(\n",
    "        RandomTreesEmbedding(n_estimators=200, max_depth=5, random_state=0),\n",
    "        TruncatedSVD(n_components=3),\n",
    "    )\n",
    "X_rte = rte.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = {\n",
    "    \"First dimension\": X_rte[:, 0],\n",
    "    \"Second dimension\": X_rte[:, 1],\n",
    "    \"Third dimension\": X_rte[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_5, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=800, size_max=2, title=\"Random Trees embedding\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## Spectral embedding\n",
    "spectral = SpectralEmbedding(\n",
    "        n_components=3, random_state=0, eigen_solver=\"arpack\"\n",
    "    )\n",
    "X_spectral = spectral.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = {\n",
    "    \"First dimension\": X_spectral[:, 0],\n",
    "    \"Second dimension\": X_spectral[:, 1],\n",
    "    \"Third dimension\": X_spectral[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_6, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=800, size_max=2, title=\"Spectral embedding\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "## T-SNE\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7 = {\n",
    "    \"First dimension\": X_tsne[:, 0],\n",
    "    \"Second dimension\": X_tsne[:, 1],\n",
    "    \"Third dimension\": X_tsne[:, 2],\n",
    "    \"colors\": y,\n",
    "    \"word\": token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_7, x=\"First dimension\", y=\"Second dimension\", z=\"Third dimension\", color=\"colors\", hover_name=\"word\", height=800, size_max=2, title=\"T-SNE\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
